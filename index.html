<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GDXSC5Y2BD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GDXSC5Y2BD');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<script src="./files/head.js"></script>

<meta name="viewport" content="width=device-width, initial-scale=1">

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta name="keywords" content="Berkeley,Deep,Reinforcement,Learning,Computer Science,Machine,Artificial,Intelligence">

<article class="post-content">
  <meta name="twitter:title" content="Reinforcement Learning as One Big Sequence Modeling Problem"/>
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:image" content="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/humanoid_padded.png" />
<article class="post-content">

<title>Reinforcement Learning as One Big Sequence Modeling Problem</title>
<link rel="stylesheet" href="./files/font.css">
<link rel="stylesheet" href="./files/main.css">

<link rel="stylesheet" type="text/css"
    href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
<style>
body {
  font-family: "Computer Modern Serif", serif;
  font-size: 14pt;
}


* {padding:0;margin:0;box-sizing:border-box;}
#video {
  position: relative;
  padding-bottom: 45%; /* 16:9 */
  height: 0;
}
#video iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 80%;
  height: 100%;
  transform: translateX(12.5%);
}

</style>

  <style type="text/css">/**
 * Style sheet used by new LibX tooltip code
 */

/* We insert a <div> with libx-tooltip style under the body.
 * This will inherit body's style - we can't afford to inherit undesirable 
 * styles and we must redefine what we need.  OTOH, some things, e.g.
 * font-size, might be ok to be inherited to stay within the page's tone.
 */
.libx-tooltip {
    display: none;
    overflow: visible;
    padding: 5px;
    z-index: 100;
    background-color: #eee;
    color: #000;
    font-weight: normal;
    font-style: normal;
    text-align: left;
    border: 2px solid #666;
    border-radius: 5px;
    -webkit-border-radius: 5px;
    -moz-border-radius: 5px;
}

.libx-tooltip p {
    /* override default 1em margin to keep paragraphs inside a tooltip closer together. */
    margin: .2em;
}
</style><style type="text/css">/**
 * Style sheet used by LibX autolinking code
 */
.libx-autolink {
}

</style>

</head>

  <body>

    <div class="outercontainer">
      <div class="container">

        <div class="content project_title">
          <br>
          <h2><a href="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/files/trajectory-transformer.pdf" style="color:black">Reinforcement Learning as One Big Sequence Modeling Problem</a></h2>
         <div class="authors">
            <a href="https://people.eecs.berkeley.edu/~janner/">Michael Janner</a>,
            <a href="https://scholar.google.com/citations?user=qlwwdfEAAAAJ&hl=en">Qiyang (Colin) Li</a>, and
            <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          </div>
        </div>

        <br><br>

        <center>
        <b><font size="4">Trajectory Transformer</font></b>
        <video width=100% height=auto autoplay playsinline muted>
          <source src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/gif_transformer.mp4" type="video/mp4">
        </video>
        <b><font size="4">Single-Step Model</font></b>
        <video width=100% height=auto autoplay playsinline muted>
          <source src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/gif_single_step.mp4" type="video/mp4">
        </video>
        <br>
        <i>Long-horizon predictions of the Trajectory Transformer compared <br>to those of a feedforward single-step dynamics model.</i>
        </center>
        <p>
        <br><br>

        <div class="content">
          <div class="text">
            <p>
              <div class="title"><b>Summary</b></div>
<!--               <b>
                <font size="5">Summary</font>
              </b> -->
              <!-- &nbsp; -->
              We view reinforcement learning as a generic sequence modeling problem and investigate how much of the usual machinery of reinforcement learning algorithms can be replaced with the tools that have found widespread use in large-scale language modeling.
              The core of our approach is the Trajectory Transformer, trained on sequences of states, actions, and rewards treated interchangeably, and a set of beam-search-based planners.
          </p>
          </div>
        </div>
        <br>
        <br>

        <div class="content">
          <div class="text">
            <p>
              <div class="title"><b>Transformers as dynamics models</b></div>
<!--               <b>
                <font size="5">Transformers as dynamics models</font>
              </b> -->
              <!-- &nbsp; -->
              Predictive dynamics models often have excellent single-step error, but poor long-horizon accuracy due to compounding errors.
              We show that Transformers are more substantially more reliable long-horizon predictors than state-of-the-art single-step models, even in continuous Markovian domains.
          </p>
          <br>
          <br>
          <center>
            <img width=25% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/markov.png">
            &nbsp;&nbsp;&nbsp;&nbsp;
            &nbsp;&nbsp;&nbsp;&nbsp;
            <img width=25% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/striated.png">
            <br>
            <i>Attention patterns of Trajectory Transformer, showing (left) a discovered <br>Markovian stratetgy and (right) an approach with action smoothing.</i>
          </center>
          <br>
          </div>
        </div>
        <br>
        <br>

        <div class="content">
          <div class="text">
            <p>
              <div class="title"><b>Beam search as trajectory optimizer</b></div>
   <!--            <b>
                <font size="5.0">
                Beam search as trajectory optimizer
              </font>
              </b> -->
               <!-- . -->
              <!-- Various control settings can be reduced to slight modifications of beam search with a sequence model. -->
              <ul>
                <li>Decoding a Trajectory Transformer with unmodified beam search gives rise to a model-based imitative method that optimizes for entire predicted trajectories to match those of an expert policy.</li>
                <li>Conditioning trajectories on a future desired state alongside previously-encountered states yields a goal-reaching method.</li>
                <br>
                <center>
                  <img width=28% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/0.png">
                  &nbsp;
                  &nbsp;
                  <img width=28% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/1.png">
                  &nbsp;
                  &nbsp;
                  <img width=28% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/2.png">
                  <br>
                  <img width=2% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/rolloutblack-1.png">
                  &nbsp;
                  Start
                  &nbsp;
                  &nbsp;
                  <img width=2% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/rolloutblue-1.png">
                  &nbsp;
                  Goal
                </center>
                <br>
                <li>Replacing log-probabilities from the sequence model with reward predictions yields a model-based planning method, surprisingly effective despite lacking the details usually required to make planning with learned models effective.</li>
                <br>
                  <center>
                    <img width=28% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/webpage_offline_halfcheetah.png">
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <img width=28% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/webpage_offline_hopper.png">
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <img width=28% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/webpage_offline_walker2d.png">
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <br>
                    <img width=50% src="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/webpage_offline_legend.png">
                  </center>
                <br>
              </ul>
          </p>
          </div>
        </div>

        <div class="content">
          <div class="text">
              <div class="title"><b>Reinforcement Learning as One Big Sequence Modeling Problem</b></div>
                 <div class="authors">
                    <a href="https://people.eecs.berkeley.edu/~janner/">Michael Janner</a>,
                    <a href="https://scholar.google.com/citations?user=qlwwdfEAAAAJ&hl=en">Qiyang (Colin) Li</a>, and
                    <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  </div>
                <div>
                  <span class="tag">
                    <a href="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/files/trajectory-transformer.pdf">Paper</a>&nbsp;
                    <a href="">Code (coming soon!)</a>&nbsp;
                  </span>
                </div>
              </li>
          </div>
        </div>

        <br><br><br>
        <div class="content">
          <div class="text">
              <div class="title"><b>Related Publication</b></div>
                Chen et al concurrently proposed another sequence modeling approach to reinforcement learning. At a high-level, ours is more model-based in spirit and theirs is more model-free, which allows us to evaluate Transformers as long-horizon dynamics models (<i>e.g.</i>, in the humanoid predictions above) and allows them to evaluate their policies in image-based environments (<i>e.g.</i>, Atari).
                <a href="https://sites.google.com/berkeley.edu/decision-transformer">We encourage you to check out their work as well.</a>
              </li>
          </div>
        </div>
 
      </div>
    </div>

<br><br><br><br>  


</div></body></html>
